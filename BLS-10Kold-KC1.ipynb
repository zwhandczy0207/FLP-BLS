{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,auc,roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = pd.read_csv(\"../dataset/NASA/CM1.csv\")\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "\n",
    "def show_accuracy(predictLabel,Label):\n",
    "    Label = np.ravel(Label).tolist()\n",
    "    predictLabel = predictLabel.tolist()\n",
    "    count = 0\n",
    "    for i in range(len(Label)):\n",
    "        if Label[i] == predictLabel[i]:\n",
    "            count += 1\n",
    "    return (round(count/len(Label),5))\n",
    "\n",
    "class node_generator(object):\n",
    "    def __init__(self, whiten = False):\n",
    "        self.Wlist = []\n",
    "        self.blist = []\n",
    "        self.function_num = 0\n",
    "        self.whiten = whiten\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def orth(self, W):\n",
    "        \"\"\"\n",
    "        目前看来，这个函数应该配合下一个generator函数是生成权重的\n",
    "        \"\"\"\n",
    "        for i in range(0, W.shape[1]):\n",
    "            w = np.mat(W[:,i].copy()).T\n",
    "            w_sum = 0\n",
    "            for j in range(i):\n",
    "                wj = np.mat(W[:,j].copy()).T\n",
    "                w_sum += (w.T.dot(wj))[0,0]*wj\n",
    "            w -= w_sum\n",
    "            w = w/np.sqrt(w.T.dot(w))\n",
    "            W[:,i] = np.ravel(w)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def generator(self, shape, times):\n",
    "        for i in range(times):\n",
    "            #random.seed(i)\n",
    "            W = 2*np.random.random(size=shape)-1\n",
    "            if self.whiten == True:\n",
    "                W = self.orth(W)   # 只在增强层使用\n",
    "            b = 2*np.random.random() -1\n",
    "            yield (W, b)\n",
    "\n",
    "    def generator_nodes(self, data, times, batchsize, function_num):\n",
    "        # 按照bls的理论，mapping layer是输入乘以不同的权重加上不同的偏差之后得到的\n",
    "        # 若干组，所以，权重是一个列表，每一个元素可作为权重与输入相乘\n",
    "        self.Wlist = [elem[0] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "        self.blist = [elem[1] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "\n",
    "        self.function_num = {'linear':self.linear,\n",
    "                        'sigmoid': self.sigmoid,\n",
    "                        'tanh':self.tanh,\n",
    "                        'relu':self.relu }[function_num]  # 激活函数供不同的层选择\n",
    "        # 下面就是先得到一组mapping nodes，再不断叠加，得到len(Wlist)组mapping nodes\n",
    "        nodes = self.function_num(data.dot(self.Wlist[0]) + self.blist[0])\n",
    "        for i in range(1, len(self.Wlist)):\n",
    "            nodes = np.column_stack((nodes, self.function_num(data.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return nodes\n",
    "\n",
    "    def transform(self,testdata):\n",
    "        testnodes = self.function_num(testdata.dot(self.Wlist[0])+self.blist[0])\n",
    "        for i in range(1,len(self.Wlist)):\n",
    "            testnodes = np.column_stack((testnodes, self.function_num(testdata.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return testnodes\n",
    "\n",
    "class scaler:\n",
    "    def __init__(self):\n",
    "        self._mean = 0\n",
    "        self._std = 0\n",
    "    \n",
    "    def fit_transform(self,traindata):\n",
    "        self._mean = traindata.mean(axis = 0)\n",
    "        self._std = traindata.std(axis = 0)\n",
    "        return (traindata-self._mean)/(self._std+0.001)\n",
    "    \n",
    "    def transform(self,testdata):\n",
    "        return (testdata-self._mean)/(self._std+0.001)\n",
    "\n",
    "class broadnet:\n",
    "    def __init__(self, \n",
    "                 maptimes = 20, \n",
    "                 enhencetimes = 20,\n",
    "                 map_function = 'linear',\n",
    "                 enhence_function = 'linear',\n",
    "                 batchsize = 'auto', \n",
    "                 reg = 0.001):\n",
    "        \n",
    "        self._maptimes = maptimes\n",
    "        self._enhencetimes = enhencetimes\n",
    "        self._batchsize = batchsize\n",
    "        self._reg = reg\n",
    "        self._map_function = map_function\n",
    "        self._enhence_function = enhence_function\n",
    "        \n",
    "        self.W = 0\n",
    "        self.pesuedoinverse = 0\n",
    "        self.normalscaler = scaler()\n",
    "        self.onehotencoder = preprocessing.OneHotEncoder(sparse = False)\n",
    "        self.mapping_generator = node_generator()\n",
    "        self.enhence_generator = node_generator(whiten = True)\n",
    "\n",
    "    def fit(self,data,label):\n",
    "        if self._batchsize == 'auto':\n",
    "            self._batchsize = data.shape[1]\n",
    "        data = self.normalscaler.fit_transform(data)\n",
    "        label = self.onehotencoder.fit_transform(np.asarray(label).reshape(-1, 1))\n",
    "        \n",
    "        mappingdata = self.mapping_generator.generator_nodes(data,self._maptimes,self._batchsize,self._map_function)\n",
    "        enhencedata = self.enhence_generator.generator_nodes(mappingdata,self._enhencetimes,self._batchsize,self._enhence_function)\n",
    "        \n",
    "        #print('number of mapping nodes {0}, number of enhence nodes {1}'.format(mappingdata.shape[1],enhencedata.shape[1]))\n",
    "        #print('mapping nodes maxvalue {0} minvalue {1} '.format(round(np.max(mappingdata),5),round(np.min(mappingdata),5)))\n",
    "        #print('enhence nodes maxvalue {0} minvalue {1} '.format(round(np.max(enhencedata),5),round(np.min(enhencedata),5)))\n",
    "        \n",
    "        inputdata = np.column_stack((mappingdata,enhencedata))\n",
    "        pesuedoinverse = self.pinv(inputdata,self._reg)\n",
    "        #print('pesuedoinverse:', pesuedoinverse)\n",
    "        self.W =  pesuedoinverse.dot(label)\n",
    "\n",
    "        \n",
    "        #print('W:', self.W)\n",
    "        #print('W:', self.W.shape)  \n",
    "    \n",
    "    def pinv(self,A,reg):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "    \n",
    "    def decode(self,Y_onehot):\n",
    "        Y = []\n",
    "        for i in range(Y_onehot.shape[0]):\n",
    "            lis = np.ravel(Y_onehot[i,:]).tolist()\n",
    "            Y.append(lis.index(max(lis)))\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def accuracy(self,predictlabel,label):\n",
    "        label = np.ravel(label).tolist()\n",
    "        predictlabel = predictlabel.tolist()\n",
    "        count = 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == predictlabel[i]:\n",
    "                count += 1\n",
    "        return (round(count/len(label),5))\n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        testdata = self.normalscaler.transform(testdata)\n",
    "        test_mappingdata = self.mapping_generator.transform(testdata)\n",
    "        test_enhencedata = self.enhence_generator.transform(test_mappingdata)\n",
    "        \n",
    "        test_inputdata = np.column_stack((test_mappingdata,test_enhencedata)) \n",
    "        #print('*predictlabel shape:',self.decode(test_inputdata.dot(self.W)).shape)\n",
    "        #print('*predictlabel:', self.decode(test_inputdata.dot(self.W)))\n",
    "        #print('*accuracy:',show_accuracy(self.decode(test_inputdata.dot(self.W)),testlabel))\n",
    "        return self.decode(test_inputdata.dot(self.W))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2109, 21) 2\n",
      "(1898, 21) (1898,) (211, 21) (211,)\n",
      "maptiems:15\tenhancetimes:15\tk_average_acc:0.78767\tk_average_f1:0.79948\tk_average_auc:0.64793\tk_average_mcc:0.26354\tk_average_Gm:0.63078\t\n",
      "maptiems:15\tenhancetimes:20\tk_average_acc:0.79056\tk_average_f1:0.80255\tk_average_auc:0.65436\tk_average_mcc:0.27545\tk_average_Gm:0.63844\t\n",
      "maptiems:15\tenhancetimes:25\tk_average_acc:0.79592\tk_average_f1:0.80729\tk_average_auc:0.6627\tk_average_mcc:0.29149\tk_average_Gm:0.64787\t\n",
      "maptiems:15\tenhancetimes:30\tk_average_acc:0.79872\tk_average_f1:0.8089\tk_average_auc:0.66155\tk_average_mcc:0.29222\tk_average_Gm:0.64584\t\n",
      "maptiems:15\tenhancetimes:35\tk_average_acc:0.80041\tk_average_f1:0.80955\tk_average_auc:0.66027\tk_average_mcc:0.2916\tk_average_Gm:0.64383\t\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(\"../../dataset/NASA/KC1.csv\")  \n",
    "  \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for item in data.columns:\n",
    "        data[item] = le.fit_transform(data[item])\n",
    "    #print(data)\n",
    "    \n",
    "    label = data['defects'].values\n",
    "    \n",
    "    data = data.drop('defects',axis=1)\n",
    "    data = data.values\n",
    "    print(data.shape,max(label)+1)\n",
    "\n",
    "    traindata,testdata,trainlabel,testlabel = train_test_split(data,label,test_size=0.1,random_state = 0)\n",
    "    print(traindata.shape,trainlabel.shape,testdata.shape,testlabel.shape)\n",
    "\n",
    "    k_acc_list, k_f1_list, k_auc_list, k_mcc_list, k_Gm_list=[],[],[],[],[]\n",
    "    #这里设置shuffle设置为ture就是打乱顺序在分配\n",
    "    kf = KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "    for map_times in np.arange(15,36,5):\n",
    "        acc_list_tmp, f1_list_tmp, auc_list_tmp, mcc_list_tmp,Gm_list_tmp=[],[],[],[],[]\n",
    "        for enhance_times in np.arange(15, 36, 5):\n",
    "            for k, (train, test) in enumerate(kf.split(traindata, trainlabel)):\n",
    "                # kf.split输出的是索引，所以由索引获取交叉后的训练集和测试集及标签\n",
    "                k_train_data,k_train_label = traindata[train], trainlabel[train]\n",
    "                k_test_data,k_test_label = traindata[test], trainlabel[test]\n",
    "                \n",
    "                bls = broadnet(maptimes = map_times, \n",
    "                           enhencetimes = enhance_times,\n",
    "                           map_function = 'relu',\n",
    "                           enhence_function = 'relu',\n",
    "                           batchsize =100,\n",
    "                           reg = 0.001)\n",
    "\n",
    "                #训练\n",
    "                starttime = datetime.datetime.now()\n",
    "                bls.fit(k_train_data,k_train_label)\n",
    "                endtime = datetime.datetime.now()\n",
    "                #print('the training time of BLS is {0} seconds'.format((endtime - starttime).total_seconds()))\n",
    "\n",
    "                #print('k_test_label:', k_test_label)\n",
    "                #预测\n",
    "                k_predict_label = bls.predict(k_test_data)\n",
    "                #print('k_predict_label:', k_predict_label)\n",
    "\n",
    "                #评价指标计算\n",
    "                acc=accuracy_score(k_test_label,k_predict_label, normalize=True)\n",
    "                fmeasure=f1_score(k_test_label,k_predict_label, average='weighted', labels=np.unique(k_test_label))\n",
    "                try:\n",
    "                    auc=roc_auc_score(k_test_label,k_predict_label, average='weighted', sample_weight=None)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                MCC=matthews_corrcoef(k_test_label,k_predict_label)\n",
    "                Gmeasure=geometric_mean_score(k_test_label,k_predict_label, average='weighted')\n",
    "\n",
    "                #将此次的十折交叉验证的结果 (10个)保存到pi_list_tmp中\n",
    "                acc_list_tmp.append(acc)\n",
    "                f1_list_tmp.append(fmeasure)\n",
    "                auc_list_tmp.append(auc)\n",
    "                mcc_list_tmp.append(MCC)\n",
    "                Gm_list_tmp.append(Gmeasure)\n",
    "                \n",
    "            #求平均保存到k_acc_list中   \n",
    "            k_average_acc=np.mean(acc_list_tmp)\n",
    "            k_average_acc=round(k_average_acc,5)\n",
    "            k_acc_list.append(k_average_acc)\n",
    "            \n",
    "            #求平均保存到k_f1_list中   \n",
    "            k_average_f1=np.mean(f1_list_tmp)\n",
    "            k_average_f1=round(k_average_f1,5)\n",
    "            k_f1_list.append(k_average_f1)\n",
    "            \n",
    "            #求平均保存到k_auc_list中   \n",
    "            k_average_auc=np.mean(auc_list_tmp)\n",
    "            k_average_auc=round(k_average_auc,5)\n",
    "            k_auc_list.append(k_average_auc)\n",
    "            \n",
    "            #求平均保存到k_mcc_list中   \n",
    "            k_average_mcc=np.mean(mcc_list_tmp)\n",
    "            k_average_mcc=round(k_average_mcc,5)\n",
    "            k_mcc_list.append(k_average_mcc)\n",
    "            \n",
    "            #求平均保存到k_Gm_list中   \n",
    "            k_average_Gm=np.mean(Gm_list_tmp)\n",
    "            k_average_Gm=round(k_average_Gm,5)\n",
    "            k_Gm_list.append(k_average_Gm)\n",
    "            print(f'maptiems:{map_times}\\tenhancetimes:{enhance_times}\\tk_average_acc:{k_average_acc}\\tk_average_f1:{k_average_f1}\\tk_average_auc:{k_average_auc}\\tk_average_mcc:{k_average_mcc}\\tk_average_Gm:{k_average_Gm}\\t')\n",
    "    \n",
    "    k_acc_array=np.array(k_acc_list)\n",
    "    k_acc_array=k_acc_array.reshape(5,5)\n",
    "    # 一维最大值索引\n",
    "    #idx_max_ravel = np.argmax(k_acc_array)\n",
    "    # true索引\n",
    "    #idx_max = np.unravel_index(idx_max_ravel, k_acc_array.shape)\n",
    "    #print('max times:',idx_max)\n",
    "    \n",
    "    k_f1_array=np.array(k_f1_list)\n",
    "    k_f1_array=k_f1_array.reshape(5,5)\n",
    "    \n",
    "    k_auc_array=np.array(k_auc_list)\n",
    "    k_auc_array=k_auc_array.reshape(5,5)\n",
    "    \n",
    "    k_mcc_array=np.array(k_mcc_list)\n",
    "    k_mcc_array=k_mcc_array.reshape(5,5)\n",
    "    \n",
    "    k_Gm_array=np.array(k_Gm_list)\n",
    "    k_Gm_array=k_Gm_array.reshape(5,5)\n",
    "    \n",
    "    sio.savemat('./data_remember/BLS_KC1_MEbest.mat',{'acc':k_acc_array,'f1':k_f1_array,'auc':k_auc_array,'mcc':k_mcc_array,'Gm':k_Gm_array})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc=accuracy_score(predictlabel, testlabel, normalize=True)\n",
    "# precision=precision_score(predictlabel, testlabel, average='weighted',zero_division=1)\n",
    "# recall=recall_score(predictlabel, testlabel, average='weighted')\n",
    "# fmeasure=f1_score(predictlabel, testlabel, average='weighted', labels=np.unique(testlabel))\n",
    "# auc=roc_auc_score(predictlabel, testlabel, average='weighted', sample_weight=None)\n",
    "# MCC=matthews_corrcoef(predictlabel, testlabel)\n",
    "# Gmeasure=geometric_mean_score(predictlabel, testlabel, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('acc：%f,precision：%f,recall：%f,fmeasure：%f,auc：%f,MCC：%f,Gmeasure：%f'%(acc,precision,recall,fmeasure,auc,MCC,Gmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
