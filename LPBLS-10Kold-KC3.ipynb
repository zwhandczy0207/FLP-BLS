{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,matthews_corrcoef\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = pd.read_csv(\"../dataset/NASA/CM1.csv\")\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "def show_accuracy(predictLabel,Label):\n",
    "    Label = np.ravel(Label).tolist()\n",
    "    predictLabel = predictLabel.tolist()\n",
    "    count = 0\n",
    "    for i in range(len(Label)):\n",
    "        if Label[i] == predictLabel[i]:\n",
    "            count += 1\n",
    "    return (round(count/len(Label),5))\n",
    "\n",
    "class node_generator(object):\n",
    "    def __init__(self, whiten = False):\n",
    "        self.Wlist = []\n",
    "        self.blist = []\n",
    "        self.function_num = 0\n",
    "        self.whiten = whiten\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def orth(self, W):\n",
    "        \"\"\"\n",
    "        目前看来，这个函数应该配合下一个generator函数是生成权重的\n",
    "        \"\"\"\n",
    "        for i in range(0, W.shape[1]):\n",
    "            w = np.mat(W[:,i].copy()).T\n",
    "            w_sum = 0\n",
    "            for j in range(i):\n",
    "                wj = np.mat(W[:,j].copy()).T\n",
    "                w_sum += (w.T.dot(wj))[0,0]*wj\n",
    "            w -= w_sum\n",
    "            w = w/np.sqrt(w.T.dot(w))\n",
    "            W[:,i] = np.ravel(w)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def generator(self, shape, times):\n",
    "        for i in range(times):\n",
    "            random.seed(i)\n",
    "            W = 2*np.random.random(size=shape)-1\n",
    "            if self.whiten == True:\n",
    "                W = self.orth(W)   # 只在增强层使用\n",
    "            b = 2*np.random.random() -1\n",
    "            yield (W, b)\n",
    "\n",
    "    def generator_nodes(self, data, times, batchsize, function_num):\n",
    "        # 按照bls的理论，mapping layer是输入乘以不同的权重加上不同的偏差之后得到的\n",
    "        # 若干组，所以，权重是一个列表，每一个元素可作为权重与输入相乘\n",
    "        self.Wlist = [elem[0] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "        self.blist = [elem[1] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "\n",
    "        self.function_num = {'linear':self.linear,\n",
    "                        'sigmoid': self.sigmoid,\n",
    "                        'tanh':self.tanh,\n",
    "                        'relu':self.relu }[function_num]  # 激活函数供不同的层选择\n",
    "        # 下面就是先得到一组mapping nodes，再不断叠加，得到len(Wlist)组mapping nodes\n",
    "        nodes = self.function_num(data.dot(self.Wlist[0]) + self.blist[0])\n",
    "        for i in range(1, len(self.Wlist)):\n",
    "            nodes = np.column_stack((nodes, self.function_num(data.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return nodes\n",
    "\n",
    "    def transform(self,testdata):\n",
    "        testnodes = self.function_num(testdata.dot(self.Wlist[0])+self.blist[0])\n",
    "        for i in range(1,len(self.Wlist)):\n",
    "            testnodes = np.column_stack((testnodes, self.function_num(testdata.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return testnodes\n",
    "\n",
    "class scaler:\n",
    "    def __init__(self):\n",
    "        self._mean = 0\n",
    "        self._std = 0\n",
    "    \n",
    "    def fit_transform(self,traindata):\n",
    "        self._mean = traindata.mean(axis = 0)\n",
    "        self._std = traindata.std(axis = 0)\n",
    "        return (traindata-self._mean)/(self._std+0.001)\n",
    "    \n",
    "    def transform(self,testdata):\n",
    "        return (testdata-self._mean)/(self._std+0.001)\n",
    "\n",
    "class broadnet:\n",
    "    def __init__(self, \n",
    "                 maptimes = 10, \n",
    "                 enhencetimes = 10,\n",
    "                 map_function = 'linear',\n",
    "                 enhence_function = 'linear',\n",
    "                 batchsize = 'auto', \n",
    "                 reg = 0.001):\n",
    "        \n",
    "        self._maptimes = maptimes\n",
    "        self._enhencetimes = enhencetimes\n",
    "        self._batchsize = batchsize\n",
    "        self._reg = reg\n",
    "        self._map_function = map_function\n",
    "        self._enhence_function = enhence_function\n",
    "        \n",
    "        self.W = 0\n",
    "        self.pesuedoinverse = 0\n",
    "        self.normalscaler = scaler()\n",
    "        self.onehotencoder = preprocessing.OneHotEncoder(sparse = False)\n",
    "        self.mapping_generator = node_generator()\n",
    "        self.enhence_generator = node_generator(whiten = True)\n",
    "\n",
    "    def fit(self,data,label,c):\n",
    "        if self._batchsize == 'auto':\n",
    "            self._batchsize = data.shape[1]\n",
    "        data = self.normalscaler.fit_transform(data)\n",
    "        label1=label\n",
    "        label = self.onehotencoder.fit_transform(np.mat(label).T)\n",
    "        \n",
    "        mappingdata = self.mapping_generator.generator_nodes(data,self._maptimes,self._batchsize,self._map_function)\n",
    "        enhencedata = self.enhence_generator.generator_nodes(mappingdata,self._enhencetimes,self._batchsize,self._enhence_function)\n",
    "        \n",
    "        #print('number of mapping nodes {0}, number of enhence nodes {1}'.format(mappingdata.shape[1],enhencedata.shape[1]))\n",
    "        #print('mapping nodes maxvalue {0} minvalue {1} '.format(round(np.max(mappingdata),5),round(np.min(mappingdata),5)))\n",
    "        #print('enhence nodes maxvalue {0} minvalue {1} '.format(round(np.max(enhencedata),5),round(np.min(enhencedata),5)))\n",
    "        \n",
    "        inputdata = np.column_stack((mappingdata,enhencedata))\n",
    "        S = self.LDA_dimensionality(inputdata,label1)\n",
    "        pesuedoinverse = self.pinv2(inputdata,self._reg,S,c)\n",
    "        self.W =  pesuedoinverse.dot(label)\n",
    "        \n",
    "        #print('W:', self.W)\n",
    "        #print('W:', self.W.shape)  \n",
    "    \n",
    "    #改写伪逆矩阵算法，将权重输入\n",
    "    def pinv(self,A,reg,weight):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(weight).dot(A)).I.dot(A.T).dot(weight)\n",
    "    \n",
    "    def pinv2(self,A,reg,S,c):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)+c*S).I.dot(A.T)\n",
    "    \n",
    "    def LDA_dimensionality(self,X, y):\n",
    "        '''\n",
    "        X为数据集，y为label，k为目标维数\n",
    "        '''\n",
    "        y=y.tolist()\n",
    "        label_ = list(set(y))\n",
    "\n",
    "        X_classify = {}\n",
    "\n",
    "        for label in label_:\n",
    "            X1 = np.array([X[i] for i in range(len(X)) if y[i] == label])\n",
    "            X_classify[label] = X1\n",
    "\n",
    "            mju = np.mean(X, axis=0)\n",
    "            mju_classify = {}\n",
    "\n",
    "        for label in label_:\n",
    "            mju1 = np.mean(X_classify[label], axis=0)\n",
    "            mju_classify[label] = mju1\n",
    "\n",
    "            #St = np.dot((X - mju).T, X - mju)\n",
    "\n",
    "            Sw = np.zeros((len(mju), len(mju)))  # 计算类内散度矩阵\n",
    "\n",
    "        for i in label_:\n",
    "            Sw += np.dot((X_classify[i] - mju_classify[i]).T,\n",
    "                         X_classify[i] - mju_classify[i])\n",
    "\n",
    "        # Sb=St-Sw\n",
    "\n",
    "        Sb = np.zeros((len(mju), len(mju)))  # 计算类间散度矩阵\n",
    "        for i in label_:\n",
    "            Sb += len(X_classify[i]) * np.dot((mju_classify[i] - mju).reshape(\n",
    "                (len(mju), 1)), (mju_classify[i] - mju).reshape((1, len(mju))))\n",
    "\n",
    "        return (Sw-Sb)\n",
    "    \n",
    "    def decode(self,Y_onehot):\n",
    "        Y = []\n",
    "        for i in range(Y_onehot.shape[0]):\n",
    "            lis = np.ravel(Y_onehot[i,:]).tolist()\n",
    "            Y.append(lis.index(max(lis)))\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def accuracy(self,predictlabel,label):\n",
    "        label = np.ravel(label).tolist()\n",
    "        predictlabel = predictlabel.tolist()\n",
    "        count = 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == predictlabel[i]:\n",
    "                count += 1\n",
    "        return (round(count/len(label),5))\n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        testdata = self.normalscaler.transform(testdata)\n",
    "        test_mappingdata = self.mapping_generator.transform(testdata)\n",
    "        test_enhencedata = self.enhence_generator.transform(test_mappingdata)\n",
    "        \n",
    "        test_inputdata = np.column_stack((test_mappingdata,test_enhencedata)) \n",
    "        #print('*predictlabel shape:',self.decode(test_inputdata.dot(self.W)).shape)\n",
    "        #print('*predictlabel:', self.decode(test_inputdata.dot(self.W)))\n",
    "        #print('*accuracy:',show_accuracy(self.decode(test_inputdata.dot(self.W)),testlabel))\n",
    "        return self.decode(test_inputdata.dot(self.W))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 39) 2\n",
      "(116, 39) (116,) (78, 39) (78,)\n",
      "maptiems:15\tenhancetimes:15\tk_average_acc:0.53106\tk_average_f1:0.57755\tk_average_auc:0.46035\tk_average_recall:0.53106\tk_average_mcc:-0.07575\tk_average_Gm:0.42253\t\n",
      "maptiems:15\tenhancetimes:20\tk_average_acc:0.53977\tk_average_f1:0.58471\tk_average_auc:0.46859\tk_average_recall:0.53977\tk_average_mcc:-0.05318\tk_average_Gm:0.4275\t\n",
      "maptiems:15\tenhancetimes:25\tk_average_acc:0.54596\tk_average_f1:0.59023\tk_average_auc:0.46878\tk_average_recall:0.54596\tk_average_mcc:-0.05584\tk_average_Gm:0.42648\t\n",
      "maptiems:15\tenhancetimes:30\tk_average_acc:0.5447\tk_average_f1:0.58837\tk_average_auc:0.46609\tk_average_recall:0.5447\tk_average_mcc:-0.06214\tk_average_Gm:0.42285\t\n",
      "maptiems:15\tenhancetimes:35\tk_average_acc:0.54409\tk_average_f1:0.58749\tk_average_auc:0.46257\tk_average_recall:0.54409\tk_average_mcc:-0.0655\tk_average_Gm:0.41951\t\n",
      "maptiems:20\tenhancetimes:15\tk_average_acc:0.53409\tk_average_f1:0.57797\tk_average_auc:0.49737\tk_average_recall:0.53409\tk_average_mcc:-0.04564\tk_average_Gm:0.46759\t\n",
      "maptiems:20\tenhancetimes:20\tk_average_acc:0.5375\tk_average_f1:0.58301\tk_average_auc:0.51131\tk_average_recall:0.5375\tk_average_mcc:-0.01512\tk_average_Gm:0.48084\t\n",
      "maptiems:20\tenhancetimes:25\tk_average_acc:0.53813\tk_average_f1:0.58418\tk_average_auc:0.49691\tk_average_recall:0.53813\tk_average_mcc:-0.02997\tk_average_Gm:0.46227\t\n",
      "maptiems:20\tenhancetimes:30\tk_average_acc:0.54962\tk_average_f1:0.59299\tk_average_auc:0.48581\tk_average_recall:0.54962\tk_average_mcc:-0.03768\tk_average_Gm:0.44421\t\n",
      "maptiems:20\tenhancetimes:35\tk_average_acc:0.55091\tk_average_f1:0.59518\tk_average_auc:0.48683\tk_average_recall:0.55091\tk_average_mcc:-0.02798\tk_average_Gm:0.44559\t\n",
      "maptiems:25\tenhancetimes:15\tk_average_acc:0.51515\tk_average_f1:0.56054\tk_average_auc:0.48903\tk_average_recall:0.51515\tk_average_mcc:-0.04308\tk_average_Gm:0.45312\t\n",
      "maptiems:25\tenhancetimes:20\tk_average_acc:0.52348\tk_average_f1:0.56718\tk_average_auc:0.47098\tk_average_recall:0.52348\tk_average_mcc:-0.06544\tk_average_Gm:0.43074\t\n",
      "maptiems:25\tenhancetimes:25\tk_average_acc:0.54066\tk_average_f1:0.58355\tk_average_auc:0.48142\tk_average_recall:0.54066\tk_average_mcc:-0.04042\tk_average_Gm:0.44365\t\n",
      "maptiems:25\tenhancetimes:30\tk_average_acc:0.54508\tk_average_f1:0.58748\tk_average_auc:0.48074\tk_average_recall:0.54508\tk_average_mcc:-0.0423\tk_average_Gm:0.44095\t\n",
      "maptiems:25\tenhancetimes:35\tk_average_acc:0.54985\tk_average_f1:0.59209\tk_average_auc:0.49094\tk_average_recall:0.54985\tk_average_mcc:-0.03029\tk_average_Gm:0.45322\t\n",
      "maptiems:30\tenhancetimes:15\tk_average_acc:0.53258\tk_average_f1:0.57642\tk_average_auc:0.50017\tk_average_recall:0.53258\tk_average_mcc:-0.01583\tk_average_Gm:0.46481\t\n",
      "maptiems:30\tenhancetimes:20\tk_average_acc:0.53598\tk_average_f1:0.58311\tk_average_auc:0.48442\tk_average_recall:0.53598\tk_average_mcc:-0.03022\tk_average_Gm:0.44719\t\n",
      "maptiems:30\tenhancetimes:25\tk_average_acc:0.53207\tk_average_f1:0.57942\tk_average_auc:0.47226\tk_average_recall:0.53207\tk_average_mcc:-0.05393\tk_average_Gm:0.43352\t\n",
      "maptiems:30\tenhancetimes:30\tk_average_acc:0.5339\tk_average_f1:0.5813\tk_average_auc:0.47439\tk_average_recall:0.5339\tk_average_mcc:-0.04155\tk_average_Gm:0.43497\t\n",
      "maptiems:30\tenhancetimes:35\tk_average_acc:0.54045\tk_average_f1:0.58664\tk_average_auc:0.47877\tk_average_recall:0.54045\tk_average_mcc:-0.03335\tk_average_Gm:0.44034\t\n",
      "maptiems:35\tenhancetimes:15\tk_average_acc:0.53182\tk_average_f1:0.57819\tk_average_auc:0.46413\tk_average_recall:0.53182\tk_average_mcc:-0.06211\tk_average_Gm:0.42459\t\n",
      "maptiems:35\tenhancetimes:20\tk_average_acc:0.52689\tk_average_f1:0.57401\tk_average_auc:0.45631\tk_average_recall:0.52689\tk_average_mcc:-0.07926\tk_average_Gm:0.41658\t\n",
      "maptiems:35\tenhancetimes:25\tk_average_acc:0.53207\tk_average_f1:0.57708\tk_average_auc:0.45651\tk_average_recall:0.53207\tk_average_mcc:-0.08012\tk_average_Gm:0.41569\t\n",
      "maptiems:35\tenhancetimes:30\tk_average_acc:0.52973\tk_average_f1:0.57644\tk_average_auc:0.45716\tk_average_recall:0.52973\tk_average_mcc:-0.06997\tk_average_Gm:0.41667\t\n",
      "maptiems:35\tenhancetimes:35\tk_average_acc:0.5353\tk_average_f1:0.58014\tk_average_auc:0.45934\tk_average_recall:0.5353\tk_average_mcc:-0.06758\tk_average_Gm:0.41764\t\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(\"../../dataset/NASA/KC3.csv\")  \n",
    "  \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for item in data.columns:\n",
    "        data[item] = le.fit_transform(data[item])\n",
    "    #print(data)\n",
    "    \n",
    "    label = data[' Defective'].values\n",
    "    \n",
    "    data = data.drop(' Defective',axis=1)\n",
    "    data = data.values\n",
    "    print(data.shape,max(label)+1)\n",
    "\n",
    "    traindata,testdata,trainlabel,testlabel = train_test_split(data,label,test_size=0.4,random_state = 0)\n",
    "    print(traindata.shape,trainlabel.shape,testdata.shape,testlabel.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    k_acc_list, k_f1_list, k_auc_list, k_recall_list, k_mcc_list, k_Gm_list=[],[],[],[],[],[]\n",
    "    #这里设置shuffle设置为ture就是打乱顺序在分配\n",
    "    kf = KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "    for map_times in np.arange(15,36,5):\n",
    "        acc_list_tmp, f1_list_tmp, auc_list_tmp, recall_list_tmp, mcc_list_tmp,Gm_list_tmp=[],[],[],[],[],[]\n",
    "        for enhance_times in np.arange(15, 36, 5):\n",
    "            for k, (train, test) in enumerate(kf.split(traindata, trainlabel)):\n",
    "                # kf.split输出的是索引，所以由索引获取交叉后的训练集和测试集及标签\n",
    "                k_train_data,k_train_label = traindata[train], trainlabel[train]\n",
    "                k_test_data,k_test_label = traindata[test], trainlabel[test]\n",
    "                \n",
    "                #k_test_label=np.transpose(k_test_label)\n",
    "                #k_train_label=np.transpose(k_train_label)\n",
    "                #k_test_label=k_test_label[0]\n",
    "                #k_train_label=k_train_label[0]\n",
    "\n",
    "                bls = broadnet(maptimes = map_times, \n",
    "                           enhencetimes = enhance_times,\n",
    "                           map_function = 'relu',\n",
    "                           enhence_function = 'relu',\n",
    "                           batchsize =100,\n",
    "                           reg = 0.001)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #训练\n",
    "                starttime = datetime.datetime.now()\n",
    "                bls.fit(k_train_data,k_train_label,1)\n",
    "                endtime = datetime.datetime.now()\n",
    "                #print('the training time of BLS is {0} seconds'.format((endtime - starttime).total_seconds()))\n",
    "\n",
    "                #print('k_test_label:', k_test_label)\n",
    "                #预测\n",
    "                k_predict_label = bls.predict(k_test_data)\n",
    "                #print('k_predict_label:', k_predict_label)\n",
    "\n",
    "                #评价指标计算\n",
    "                acc=accuracy_score(k_test_label,k_predict_label, normalize=True)\n",
    "                fmeasure=f1_score(k_test_label,k_predict_label, average='weighted', labels=np.unique(k_test_label))\n",
    "                try:\n",
    "                    auc=roc_auc_score(k_test_label,k_predict_label, average='weighted', sample_weight=None)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                recall=recall_score(k_test_label, k_predict_label, average='weighted')\n",
    "                MCC=matthews_corrcoef(k_test_label,k_predict_label)\n",
    "                Gmeasure=geometric_mean_score(k_test_label,k_predict_label, average='weighted')\n",
    "\n",
    "                #将此次的十折交叉验证的结果 (10个)保存到pi_list_tmp中\n",
    "                acc_list_tmp.append(acc)\n",
    "                f1_list_tmp.append(fmeasure)\n",
    "                auc_list_tmp.append(auc)\n",
    "                recall_list_tmp.append(recall)\n",
    "                mcc_list_tmp.append(MCC)\n",
    "                Gm_list_tmp.append(Gmeasure)\n",
    "                \n",
    "            #求平均保存到k_acc_list中   \n",
    "            k_average_acc=np.mean(acc_list_tmp)\n",
    "            k_average_acc=round(k_average_acc,5)\n",
    "            k_acc_list.append(k_average_acc)\n",
    "            \n",
    "            #求平均保存到k_f1_list中   \n",
    "            k_average_f1=np.mean(f1_list_tmp)\n",
    "            k_average_f1=round(k_average_f1,5)\n",
    "            k_f1_list.append(k_average_f1)\n",
    "            \n",
    "            #求平均保存到k_auc_list中   \n",
    "            k_average_auc=np.mean(auc_list_tmp)\n",
    "            k_average_auc=round(k_average_auc,5)\n",
    "            k_auc_list.append(k_average_auc)\n",
    "            \n",
    "            #求平均保存到k_recall_list中   \n",
    "            k_average_recall=np.mean(recall_list_tmp)\n",
    "            k_average_recall=round(k_average_recall,5)\n",
    "            k_recall_list.append(k_average_recall)\n",
    "            \n",
    "            #求平均保存到k_mcc_list中   \n",
    "            k_average_mcc=np.mean(mcc_list_tmp)\n",
    "            k_average_mcc=round(k_average_mcc,5)\n",
    "            k_mcc_list.append(k_average_mcc)\n",
    "            \n",
    "            #求平均保存到k_Gm_list中   \n",
    "            k_average_Gm=np.mean(Gm_list_tmp)\n",
    "            k_average_Gm=round(k_average_Gm,5)\n",
    "            k_Gm_list.append(k_average_Gm)\n",
    "            print(f'maptiems:{map_times}\\tenhancetimes:{enhance_times}\\tk_average_acc:{k_average_acc}\\tk_average_f1:{k_average_f1}\\tk_average_auc:{k_average_auc}\\tk_average_recall:{k_average_recall}\\tk_average_mcc:{k_average_mcc}\\tk_average_Gm:{k_average_Gm}\\t')\n",
    "\n",
    "    k_acc_array=np.array(k_acc_list)\n",
    "    k_acc_array=k_acc_array.reshape(5,5)\n",
    "    # 一维最大值索引\n",
    "    #idx_max_ravel = np.argmax(k_acc_array)\n",
    "    # true索引\n",
    "    #idx_max = np.unravel_index(idx_max_ravel, k_acc_array.shape)\n",
    "    #print('max times:',idx_max)\n",
    "    \n",
    "    k_f1_array=np.array(k_f1_list)\n",
    "    k_f1_array=k_f1_array.reshape(5,5)\n",
    "    \n",
    "    k_auc_array=np.array(k_auc_list)\n",
    "    k_auc_array=k_auc_array.reshape(5,5)\n",
    "    \n",
    "    k_recall_array=np.array(k_recall_list)\n",
    "    k_recall_array=k_recall_array.reshape(5,5)\n",
    "    \n",
    "    k_mcc_array=np.array(k_mcc_list)\n",
    "    k_mcc_array=k_mcc_array.reshape(5,5)\n",
    "    \n",
    "    k_Gm_array=np.array(k_Gm_list)\n",
    "    k_Gm_array=k_Gm_array.reshape(5,5)\n",
    "    \n",
    "    sio.savemat('./data_remember/0.75LPBLS_KC3_MEbest.mat',{'acc':k_acc_array,'f1':k_f1_array,'auc':k_auc_array,'recall':k_recall_array,'mcc':k_mcc_array,'Gm':k_Gm_array})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc=accuracy_score(predictlabel, testlabel, normalize=True)\n",
    "# precision=precision_score(predictlabel, testlabel, average='weighted',zero_division=1)\n",
    "# recall=recall_score(predictlabel, testlabel, average='weighted')\n",
    "# fmeasure=f1_score(predictlabel, testlabel, average='weighted', labels=np.unique(testlabel))\n",
    "# auc=roc_auc_score(predictlabel, testlabel, average='weighted', sample_weight=None)\n",
    "# MCC=matthews_corrcoef(predictlabel, testlabel)\n",
    "# Gmeasure=geometric_mean_score(predictlabel, testlabel, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('acc：%f,precision：%f,recall：%f,fmeasure：%f,auc：%f,,MCC：%f,Gmeasure：%f'%(acc,precision,recall,fmeasure,auc,MCC,Gmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf01",
   "language": "python",
   "name": "tf01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
