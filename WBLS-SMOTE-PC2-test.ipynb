{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score,roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"../../dataset/NASA/PC2.csv\")\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRANCH_COUNT</th>\n",
       "      <th>CALL_PAIRS</th>\n",
       "      <th>LOC_CODE_AND_COMMENT</th>\n",
       "      <th>LOC_COMMENTS</th>\n",
       "      <th>CONDITION_COUNT</th>\n",
       "      <th>CYCLOMATIC_COMPLEXITY</th>\n",
       "      <th>CYCLOMATIC_DENSITY</th>\n",
       "      <th>DECISION_COUNT</th>\n",
       "      <th>DECISION_DENSITY</th>\n",
       "      <th>DESIGN_COMPLEXITY</th>\n",
       "      <th>...</th>\n",
       "      <th>MULTIPLE_CONDITION_COUNT</th>\n",
       "      <th>NODE_COUNT</th>\n",
       "      <th>NORMALIZED_CYLOMATIC_COMPLEXITY</th>\n",
       "      <th>NUM_OPERANDS</th>\n",
       "      <th>NUM_OPERATORS</th>\n",
       "      <th>NUM_UNIQUE_OPERANDS</th>\n",
       "      <th>NUM_UNIQUE_OPERATORS</th>\n",
       "      <th>NUMBER_OF_LINES</th>\n",
       "      <th>PERCENT_COMMENTS</th>\n",
       "      <th>LOC_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.850636</td>\n",
       "      <td>2.945077</td>\n",
       "      <td>9.770261</td>\n",
       "      <td>3.058272</td>\n",
       "      <td>9.259210</td>\n",
       "      <td>3.474213</td>\n",
       "      <td>0.445693</td>\n",
       "      <td>4.392498</td>\n",
       "      <td>2.116048</td>\n",
       "      <td>2.694575</td>\n",
       "      <td>...</td>\n",
       "      <td>4.630275</td>\n",
       "      <td>13.087743</td>\n",
       "      <td>0.275177</td>\n",
       "      <td>21.656397</td>\n",
       "      <td>31.983925</td>\n",
       "      <td>9.231078</td>\n",
       "      <td>10.884796</td>\n",
       "      <td>17.373074</td>\n",
       "      <td>76.033369</td>\n",
       "      <td>11.864032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.876981</td>\n",
       "      <td>4.197814</td>\n",
       "      <td>21.644059</td>\n",
       "      <td>9.979131</td>\n",
       "      <td>18.309096</td>\n",
       "      <td>5.073736</td>\n",
       "      <td>0.236106</td>\n",
       "      <td>8.864711</td>\n",
       "      <td>0.328705</td>\n",
       "      <td>4.120888</td>\n",
       "      <td>...</td>\n",
       "      <td>9.154978</td>\n",
       "      <td>20.526278</td>\n",
       "      <td>0.116308</td>\n",
       "      <td>42.766455</td>\n",
       "      <td>62.230166</td>\n",
       "      <td>13.196272</td>\n",
       "      <td>4.807430</td>\n",
       "      <td>31.859582</td>\n",
       "      <td>13.509902</td>\n",
       "      <td>24.711057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>85.710000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>287.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>558.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>570.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>843.000000</td>\n",
       "      <td>1198.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>98.250000</td>\n",
       "      <td>663.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BRANCH_COUNT   CALL_PAIRS  LOC_CODE_AND_COMMENT  LOC_COMMENTS  \\\n",
       "count   1493.000000  1493.000000           1493.000000   1493.000000   \n",
       "mean       5.850636     2.945077              9.770261      3.058272   \n",
       "std        9.876981     4.197814             21.644059      9.979131   \n",
       "min        1.000000     0.000000              0.000000      0.000000   \n",
       "25%        3.000000     1.000000              3.000000      0.000000   \n",
       "50%        3.000000     2.000000              5.000000      0.000000   \n",
       "75%        7.000000     4.000000             11.000000      1.000000   \n",
       "max      287.000000    74.000000            558.000000    111.000000   \n",
       "\n",
       "       CONDITION_COUNT  CYCLOMATIC_COMPLEXITY  CYCLOMATIC_DENSITY  \\\n",
       "count      1493.000000            1493.000000         1493.000000   \n",
       "mean          9.259210               3.474213            0.445693   \n",
       "std          18.309096               5.073736            0.236106   \n",
       "min           4.000000               1.000000            0.030000   \n",
       "25%           4.000000               2.000000            0.270000   \n",
       "50%           6.000000               2.000000            0.400000   \n",
       "75%          12.000000               4.000000            0.570000   \n",
       "max         570.000000             144.000000            1.000000   \n",
       "\n",
       "       DECISION_COUNT  DECISION_DENSITY  DESIGN_COMPLEXITY  ...  \\\n",
       "count     1493.000000       1493.000000        1493.000000  ...   \n",
       "mean         4.392498          2.116048           2.694575  ...   \n",
       "std          8.864711          0.328705           4.120888  ...   \n",
       "min          2.000000          2.000000           1.000000  ...   \n",
       "25%          2.000000          2.000000           1.000000  ...   \n",
       "50%          2.000000          2.000000           2.000000  ...   \n",
       "75%          6.000000          2.000000           3.000000  ...   \n",
       "max        284.000000          7.000000         115.000000  ...   \n",
       "\n",
       "       MULTIPLE_CONDITION_COUNT   NODE_COUNT  NORMALIZED_CYLOMATIC_COMPLEXITY  \\\n",
       "count               1493.000000  1493.000000                      1493.000000   \n",
       "mean                   4.630275    13.087743                         0.275177   \n",
       "std                    9.154978    20.526278                         0.116308   \n",
       "min                    2.000000     2.000000                         0.020000   \n",
       "25%                    2.000000     6.000000                         0.200000   \n",
       "50%                    3.000000     8.000000                         0.250000   \n",
       "75%                    6.000000    14.000000                         0.330000   \n",
       "max                  285.000000   568.000000                         0.600000   \n",
       "\n",
       "       NUM_OPERANDS  NUM_OPERATORS  NUM_UNIQUE_OPERANDS  NUM_UNIQUE_OPERATORS  \\\n",
       "count   1493.000000    1493.000000          1493.000000           1493.000000   \n",
       "mean      21.656397      31.983925             9.231078             10.884796   \n",
       "std       42.766455      62.230166            13.196272              4.807430   \n",
       "min        1.000000       4.000000             1.000000              4.000000   \n",
       "25%        6.000000      10.000000             4.000000              7.000000   \n",
       "50%       11.000000      15.000000             7.000000              9.000000   \n",
       "75%       24.000000      34.000000            11.000000             14.000000   \n",
       "max      843.000000    1198.000000           245.000000             46.000000   \n",
       "\n",
       "       NUMBER_OF_LINES  PERCENT_COMMENTS    LOC_TOTAL  \n",
       "count      1493.000000       1493.000000  1493.000000  \n",
       "mean         17.373074         76.033369    11.864032  \n",
       "std          31.859582         13.509902    24.711057  \n",
       "min           4.000000          0.000000     2.000000  \n",
       "25%           6.000000         70.000000     4.000000  \n",
       "50%          10.000000         80.000000     7.000000  \n",
       "75%          18.000000         85.710000    13.000000  \n",
       "max         776.000000         98.250000   663.000000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1493, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 14, 0, 12, 4, 0.24, 6, 2.0, 2, 0.5, 15, 1, 0, 3, 5, 15.95,\n",
       "       27.3, 11891.1, 0.15, 95, 0.04, 660.62, 435.57, 0.25, 3, 6, 13, 0.2,\n",
       "       39, 56, 10, 14, 20, 82.35, 17, 0], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=data1.values\n",
    "for i in range(1493):\n",
    "    if data2[i][36]==\"Y\":\n",
    "        data2[i][36]=1\n",
    "    else:\n",
    "        data2[i][36]=0\n",
    "data2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(data2).astype(float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  1.  , 14.  , ..., 20.  , 82.35, 17.  ],\n",
       "       [ 7.  ,  2.  , 14.  , ..., 20.  , 82.35, 17.  ],\n",
       "       [ 7.  ,  0.  ,  4.  , ...,  8.  , 80.  ,  5.  ],\n",
       "       ...,\n",
       "       [ 3.  ,  6.  , 32.  , ..., 46.  , 92.86, 35.  ],\n",
       "       [ 5.  ,  3.  , 16.  , ..., 29.  , 88.89, 19.  ],\n",
       "       [ 7.  ,  0.  , 15.  , ..., 23.  , 76.19, 20.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=X[:,36]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,0:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2954, 36), (2954,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "#os =  RandomOverSampler()\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n",
    "X_res.shape,y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2954, 36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape\n",
    "#X_res=X_res.reshape(898,21,1)\n",
    "#X_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def show_accuracy(predictLabel,Label):\n",
    "    Label = np.ravel(Label).tolist()\n",
    "    predictLabel = predictLabel.tolist()\n",
    "    count = 0\n",
    "    for i in range(len(Label)):\n",
    "        if Label[i] == predictLabel[i]:\n",
    "            count += 1\n",
    "    return (round(count/len(Label),5))\n",
    "\n",
    "class node_generator(object):\n",
    "    def __init__(self, whiten = False):\n",
    "        self.Wlist = []\n",
    "        self.blist = []\n",
    "        self.function_num = 0\n",
    "        self.whiten = whiten\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def orth(self, W):\n",
    "        \"\"\"\n",
    "        目前看来，这个函数应该配合下一个generator函数是生成权重的\n",
    "        \"\"\"\n",
    "        for i in range(0, W.shape[1]):\n",
    "            w = np.mat(W[:,i].copy()).T\n",
    "            w_sum = 0\n",
    "            for j in range(i):\n",
    "                wj = np.mat(W[:,j].copy()).T\n",
    "                w_sum += (w.T.dot(wj))[0,0]*wj\n",
    "            w -= w_sum\n",
    "            w = w/np.sqrt(w.T.dot(w))\n",
    "            W[:,i] = np.ravel(w)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def generator(self, shape, times):\n",
    "        for i in range(times):\n",
    "            random.seed(i)\n",
    "            W = 2*np.random.random(size=shape)-1\n",
    "            if self.whiten == True:\n",
    "                W = self.orth(W)   # 只在增强层使用\n",
    "            b = 2*np.random.random() -1\n",
    "            yield (W, b)\n",
    "\n",
    "    def generator_nodes(self, data, times, batchsize, function_num):\n",
    "        # 按照bls的理论，mapping layer是输入乘以不同的权重加上不同的偏差之后得到的\n",
    "        # 若干组，所以，权重是一个列表，每一个元素可作为权重与输入相乘\n",
    "        self.Wlist = [elem[0] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "        self.blist = [elem[1] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "\n",
    "        self.function_num = {'linear':self.linear,\n",
    "                        'sigmoid': self.sigmoid,\n",
    "                        'tanh':self.tanh,\n",
    "                        'relu':self.relu }[function_num]  # 激活函数供不同的层选择\n",
    "        # 下面就是先得到一组mapping nodes，再不断叠加，得到len(Wlist)组mapping nodes\n",
    "        nodes = self.function_num(data.dot(self.Wlist[0]) + self.blist[0])\n",
    "        for i in range(1, len(self.Wlist)):\n",
    "            nodes = np.column_stack((nodes, self.function_num(data.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return nodes\n",
    "\n",
    "    def transform(self,testdata):\n",
    "        testnodes = self.function_num(testdata.dot(self.Wlist[0])+self.blist[0])\n",
    "        for i in range(1,len(self.Wlist)):\n",
    "            testnodes = np.column_stack((testnodes, self.function_num(testdata.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return testnodes\n",
    "\n",
    "class scaler:\n",
    "    def __init__(self):\n",
    "        self._mean = 0\n",
    "        self._std = 0\n",
    "    \n",
    "    def fit_transform(self,traindata):\n",
    "        self._mean = traindata.mean(axis = 0)\n",
    "        self._std = traindata.std(axis = 0)\n",
    "        return (traindata-self._mean)/(self._std+0.001)\n",
    "    \n",
    "    def transform(self,testdata):\n",
    "        return (testdata-self._mean)/(self._std+0.001)\n",
    "\n",
    "class broadnet:\n",
    "    def __init__(self, \n",
    "                 maptimes = 10, \n",
    "                 enhencetimes = 10,\n",
    "                 map_function = 'linear',\n",
    "                 enhence_function = 'linear',\n",
    "                 batchsize = 'auto', \n",
    "                 reg = 0.001):\n",
    "        \n",
    "        self._maptimes = maptimes\n",
    "        self._enhencetimes = enhencetimes\n",
    "        self._batchsize = batchsize\n",
    "        self._reg = reg\n",
    "        self._map_function = map_function\n",
    "        self._enhence_function = enhence_function\n",
    "        \n",
    "        self.W = 0\n",
    "        self.pesuedoinverse = 0\n",
    "        self.normalscaler = scaler()\n",
    "        self.onehotencoder = preprocessing.OneHotEncoder(sparse = False)\n",
    "        self.mapping_generator = node_generator()\n",
    "        self.enhence_generator = node_generator(whiten = True)\n",
    "\n",
    "    def fit(self,data,label,weight):\n",
    "        if self._batchsize == 'auto':\n",
    "            self._batchsize = data.shape[1]\n",
    "        data = self.normalscaler.fit_transform(data)\n",
    "        label = self.onehotencoder.fit_transform(np.mat(label).T)\n",
    "        \n",
    "        mappingdata = self.mapping_generator.generator_nodes(data,self._maptimes,self._batchsize,self._map_function)\n",
    "        enhencedata = self.enhence_generator.generator_nodes(mappingdata,self._enhencetimes,self._batchsize,self._enhence_function)\n",
    "        \n",
    "        #print('number of mapping nodes {0}, number of enhence nodes {1}'.format(mappingdata.shape[1],enhencedata.shape[1]))\n",
    "        #print('mapping nodes maxvalue {0} minvalue {1} '.format(round(np.max(mappingdata),5),round(np.min(mappingdata),5)))\n",
    "        #print('enhence nodes maxvalue {0} minvalue {1} '.format(round(np.max(enhencedata),5),round(np.min(enhencedata),5)))\n",
    "        \n",
    "        inputdata = np.column_stack((mappingdata,enhencedata))\n",
    "        pesuedoinverse = self.pinv(inputdata,self._reg,weight)\n",
    "        self.W =  pesuedoinverse.dot(label)\n",
    "        \n",
    "        #print('W:', self.W)\n",
    "        #print('W:', self.W.shape)  \n",
    "    \n",
    "    #改写伪逆矩阵算法，将权重输入\n",
    "    def pinv(self,A,reg,weight):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(weight).dot(A)).I.dot(A.T).dot(weight)\n",
    "    \n",
    "    def decode(self,Y_onehot):\n",
    "        Y = []\n",
    "        for i in range(Y_onehot.shape[0]):\n",
    "            lis = np.ravel(Y_onehot[i,:]).tolist()\n",
    "            Y.append(lis.index(max(lis)))\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def accuracy(self,predictlabel,label):\n",
    "        label = np.ravel(label).tolist()\n",
    "        predictlabel = predictlabel.tolist()\n",
    "        count = 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == predictlabel[i]:\n",
    "                count += 1\n",
    "        return (round(count/len(label),5))\n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        testdata = self.normalscaler.transform(testdata)\n",
    "        test_mappingdata = self.mapping_generator.transform(testdata)\n",
    "        test_enhencedata = self.enhence_generator.transform(test_mappingdata)\n",
    "        \n",
    "        test_inputdata = np.column_stack((test_mappingdata,test_enhencedata)) \n",
    "        #print('*predictlabel shape:',self.decode(test_inputdata.dot(self.W)).shape)\n",
    "        #print('*predictlabel:', self.decode(test_inputdata.dot(self.W)))\n",
    "        #print('*accuracy:',show_accuracy(self.decode(test_inputdata.dot(self.W)),testlabel))\n",
    "        return self.decode(test_inputdata.dot(self.W))\n",
    "    \n",
    "    def addWeight(self,trainlabel):\n",
    "        #对BLS设置加权，按照训练集中的label比例进行权重设置\n",
    "        #求训练数据集中label为1的个数\n",
    "        count=0\n",
    "        for z in range(len(trainlabel)):\n",
    "            #print ('个数 %d ' %z)\n",
    "            #print('k_train_label:', k_train_label[z])\n",
    "            if trainlabel[z]==1:\n",
    "                count=count+1\n",
    "        #print ('label为1的个数 %d ' %j)\n",
    "        #print(count)\n",
    "        #print(k_train_label.shape)\n",
    "        #print(len(k_train_label))\n",
    "        #权重设置 \n",
    "        weight = np.zeros((len(trainlabel),len(trainlabel)))\n",
    "        for i in range(len(trainlabel)):\n",
    "            if trainlabel[i]==1:\n",
    "                #weight[i,i]=0.618/count\n",
    "                weight[i,i]=2*(len(trainlabel)-count)/len(trainlabel)\n",
    "            else:\n",
    "                #weight[i,i]=1/(len(k_train_label)-count)\n",
    "                weight[i,i]=2*count/len(trainlabel)\n",
    "        #print('Weight:', weight)\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集划分为训练集和测试集\n",
    "#traindata,testdata,trainlabel,testlabel = train_test_split(X_res,y_res,test_size=0.2,random_state = 0)\n",
    "#print(traindata.shape,trainlabel.shape,testdata.shape,testlabel.shape)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2)\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingTime：4.352668,acc：0.993234,fmeasure：0.993236,auc：0.993590,recall：0.993234,MCC：0.986527,Gmeasure：0.993590\n",
      "trainingTime：4.344022,acc：0.993234,fmeasure：0.993233,auc：0.993113,recall：0.993234,MCC：0.986551,Gmeasure：0.993113\n",
      "trainingTime：4.256550,acc：0.991881,fmeasure：0.991878,auc：0.991573,recall：0.991881,MCC：0.983861,Gmeasure：0.991573\n",
      "trainingTime：4.278584,acc：0.993234,fmeasure：0.993233,auc：0.993151,recall：0.993234,MCC：0.986554,Gmeasure：0.993151\n",
      "trainingTime：4.307866,acc：0.987821,fmeasure：0.987820,auc：0.987871,recall：0.987821,MCC：0.975934,Gmeasure：0.987871\n",
      "trainingTime：4.321319,acc：0.991881,fmeasure：0.991879,auc：0.991690,recall：0.991881,MCC：0.983877,Gmeasure：0.991690\n",
      "trainingTime：4.242882,acc：0.991881,fmeasure：0.991879,auc：0.991758,recall：0.991881,MCC：0.983884,Gmeasure：0.991758\n",
      "trainingTime：4.253216,acc：0.997294,fmeasure：0.997294,auc：0.997312,recall：0.997294,MCC：0.994602,Gmeasure：0.997312\n",
      "trainingTime：4.235443,acc：0.997294,fmeasure：0.997294,auc：0.997253,recall：0.997294,MCC：0.994600,Gmeasure：0.997253\n",
      "trainingTime：4.227867,acc：0.987821,fmeasure：0.987825,auc：0.988220,recall：0.987821,MCC：0.975925,Gmeasure：0.988220\n"
     ]
    }
   ],
   "source": [
    "Evaluat_list=[]\n",
    "for i in np.arange(0,10):\n",
    "    #将数据集划分为训练集和测试集\n",
    "    traindata,testdata,trainlabel,testlabel = train_test_split(X_res,y_res,test_size=0.25,random_state = i)\n",
    "    bls = broadnet(maptimes = 30, \n",
    "               enhencetimes = 15,\n",
    "               map_function = 'relu',\n",
    "               enhence_function = 'relu',\n",
    "               batchsize =100,\n",
    "               reg = 0.001)\n",
    "\n",
    "    #根据训练数据设置权重\n",
    "    weight=bls.addWeight(trainlabel)\n",
    "\n",
    "    #训练\n",
    "    starttime = datetime.datetime.now()\n",
    "    bls.fit(traindata,trainlabel,weight)\n",
    "    endtime = datetime.datetime.now()\n",
    "    trainingTime=(endtime - starttime).total_seconds()\n",
    "    #print('the training time of BLS is {0} seconds'.format((endtime - starttime).total_seconds()))\n",
    "\n",
    "    #预测\n",
    "    predictlabel = bls.predict(testdata)\n",
    "\n",
    "    #评价指标计算\n",
    "    acc=accuracy_score(testlabel,predictlabel,normalize=True)\n",
    "    fmeasure=f1_score(testlabel,predictlabel, average='weighted', labels=np.unique(testlabel))\n",
    "    try:\n",
    "        auc=roc_auc_score(testlabel,predictlabel, average='macro', sample_weight=None)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    MCC=matthews_corrcoef(testlabel,predictlabel)\n",
    "    Gmeasure=geometric_mean_score(testlabel,predictlabel, average='weighted')\n",
    "    recall=recall_score(testlabel, predictlabel, average='weighted')\n",
    "\n",
    "    print('trainingTime：%f,acc：%f,fmeasure：%f,auc：%f,recall：%f,MCC：%f,Gmeasure：%f'%(trainingTime,acc,fmeasure,auc,recall,MCC,Gmeasure))\n",
    "    Evaluat_tuple=(trainingTime,acc,fmeasure,auc,recall,MCC,Gmeasure)    \n",
    "    Evaluat_list.append(Evaluat_tuple)\n",
    "\n",
    "# 表头\n",
    "header = ['trainTime', 'acc', 'fmeasure', 'auc','recall', 'MCC', 'Gmeasure']\n",
    "with open('./data_remember/0.75WBLS_Smote_PC2_Evaluat.csv', 'w', encoding='utf-8', newline='') as file_obj:\n",
    "    # 创建对象\n",
    "    writer = csv.writer(file_obj)\n",
    "    # 写表头\n",
    "    writer.writerow(header)\n",
    "    # 3.写入数据(一次性写入多行)\n",
    "    writer.writerows(Evaluat_list)\n",
    "\n",
    "\n",
    "    #sio.savemat('./data_remember/WBLS_Smote_CM1_test.mat',{'time':format((endtime - starttime).total_seconds()),'acc':acc,'f1':fmeasure,'auc':auc,'mcc':MCC,'Gm':Gmeasure})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf01",
   "language": "python",
   "name": "tf01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
