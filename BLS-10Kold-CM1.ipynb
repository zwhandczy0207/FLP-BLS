{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,auc,roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = pd.read_csv(\"../dataset/NASA/CM1.csv\")\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "\n",
    "def show_accuracy(predictLabel,Label):\n",
    "    Label = np.ravel(Label).tolist()\n",
    "    predictLabel = predictLabel.tolist()\n",
    "    count = 0\n",
    "    for i in range(len(Label)):\n",
    "        if Label[i] == predictLabel[i]:\n",
    "            count += 1\n",
    "    return (round(count/len(Label),5))\n",
    "\n",
    "class node_generator(object):\n",
    "    def __init__(self, whiten = False):\n",
    "        self.Wlist = []\n",
    "        self.blist = []\n",
    "        self.function_num = 0\n",
    "        self.whiten = whiten\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def orth(self, W):\n",
    "        \"\"\"\n",
    "        目前看来，这个函数应该配合下一个generator函数是生成权重的\n",
    "        \"\"\"\n",
    "        for i in range(0, W.shape[1]):\n",
    "            w = np.mat(W[:,i].copy()).T\n",
    "            w_sum = 0\n",
    "            for j in range(i):\n",
    "                wj = np.mat(W[:,j].copy()).T\n",
    "                w_sum += (w.T.dot(wj))[0,0]*wj\n",
    "            w -= w_sum\n",
    "            w = w/np.sqrt(w.T.dot(w))\n",
    "            W[:,i] = np.ravel(w)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def generator(self, shape, times):\n",
    "        for i in range(times):\n",
    "            #random.seed(i)\n",
    "            W = 2*np.random.random(size=shape)-1\n",
    "            if self.whiten == True:\n",
    "                W = self.orth(W)   # 只在增强层使用\n",
    "            b = 2*np.random.random() -1\n",
    "            yield (W, b)\n",
    "\n",
    "    def generator_nodes(self, data, times, batchsize, function_num):\n",
    "        # 按照bls的理论，mapping layer是输入乘以不同的权重加上不同的偏差之后得到的\n",
    "        # 若干组，所以，权重是一个列表，每一个元素可作为权重与输入相乘\n",
    "        self.Wlist = [elem[0] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "        self.blist = [elem[1] for elem in self.generator((data.shape[1], batchsize), times)]\n",
    "\n",
    "        self.function_num = {'linear':self.linear,\n",
    "                        'sigmoid': self.sigmoid,\n",
    "                        'tanh':self.tanh,\n",
    "                        'relu':self.relu }[function_num]  # 激活函数供不同的层选择\n",
    "        # 下面就是先得到一组mapping nodes，再不断叠加，得到len(Wlist)组mapping nodes\n",
    "        nodes = self.function_num(data.dot(self.Wlist[0]) + self.blist[0])\n",
    "        for i in range(1, len(self.Wlist)):\n",
    "            nodes = np.column_stack((nodes, self.function_num(data.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return nodes\n",
    "\n",
    "    def transform(self,testdata):\n",
    "        testnodes = self.function_num(testdata.dot(self.Wlist[0])+self.blist[0])\n",
    "        for i in range(1,len(self.Wlist)):\n",
    "            testnodes = np.column_stack((testnodes, self.function_num(testdata.dot(self.Wlist[i])+self.blist[i])))\n",
    "        return testnodes\n",
    "\n",
    "class scaler:\n",
    "    def __init__(self):\n",
    "        self._mean = 0\n",
    "        self._std = 0\n",
    "    \n",
    "    def fit_transform(self,traindata):\n",
    "        self._mean = traindata.mean(axis = 0)\n",
    "        self._std = traindata.std(axis = 0)\n",
    "        return (traindata-self._mean)/(self._std+0.001)\n",
    "    \n",
    "    def transform(self,testdata):\n",
    "        return (testdata-self._mean)/(self._std+0.001)\n",
    "\n",
    "class broadnet:\n",
    "    def __init__(self, \n",
    "                 maptimes = 20, \n",
    "                 enhencetimes = 20,\n",
    "                 map_function = 'linear',\n",
    "                 enhence_function = 'linear',\n",
    "                 batchsize = 'auto', \n",
    "                 reg = 0.001):\n",
    "        \n",
    "        self._maptimes = maptimes\n",
    "        self._enhencetimes = enhencetimes\n",
    "        self._batchsize = batchsize\n",
    "        self._reg = reg\n",
    "        self._map_function = map_function\n",
    "        self._enhence_function = enhence_function\n",
    "        \n",
    "        self.W = 0\n",
    "        self.pesuedoinverse = 0\n",
    "        self.normalscaler = scaler()\n",
    "        self.onehotencoder = preprocessing.OneHotEncoder(sparse = False)\n",
    "        self.mapping_generator = node_generator()\n",
    "        self.enhence_generator = node_generator(whiten = True)\n",
    "\n",
    "    def fit(self,data,label):\n",
    "        if self._batchsize == 'auto':\n",
    "            self._batchsize = data.shape[1]\n",
    "        data = self.normalscaler.fit_transform(data)\n",
    "        label = self.onehotencoder.fit_transform(np.asarray(label).reshape(-1, 1))\n",
    "        \n",
    "        mappingdata = self.mapping_generator.generator_nodes(data,self._maptimes,self._batchsize,self._map_function)\n",
    "        enhencedata = self.enhence_generator.generator_nodes(mappingdata,self._enhencetimes,self._batchsize,self._enhence_function)\n",
    "        \n",
    "        #print('number of mapping nodes {0}, number of enhence nodes {1}'.format(mappingdata.shape[1],enhencedata.shape[1]))\n",
    "        #print('mapping nodes maxvalue {0} minvalue {1} '.format(round(np.max(mappingdata),5),round(np.min(mappingdata),5)))\n",
    "        #print('enhence nodes maxvalue {0} minvalue {1} '.format(round(np.max(enhencedata),5),round(np.min(enhencedata),5)))\n",
    "        \n",
    "        inputdata = np.column_stack((mappingdata,enhencedata))\n",
    "        pesuedoinverse = self.pinv(inputdata,self._reg)\n",
    "        #print('pesuedoinverse:', pesuedoinverse)\n",
    "        self.W =  pesuedoinverse.dot(label)\n",
    "\n",
    "        \n",
    "        #print('W:', self.W)\n",
    "        #print('W:', self.W.shape)  \n",
    "    \n",
    "    def pinv(self,A,reg):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "    \n",
    "    def decode(self,Y_onehot):\n",
    "        Y = []\n",
    "        for i in range(Y_onehot.shape[0]):\n",
    "            lis = np.ravel(Y_onehot[i,:]).tolist()\n",
    "            Y.append(lis.index(max(lis)))\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def accuracy(self,predictlabel,label):\n",
    "        label = np.ravel(label).tolist()\n",
    "        predictlabel = predictlabel.tolist()\n",
    "        count = 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == predictlabel[i]:\n",
    "                count += 1\n",
    "        return (round(count/len(label),5))\n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        testdata = self.normalscaler.transform(testdata)\n",
    "        test_mappingdata = self.mapping_generator.transform(testdata)\n",
    "        test_enhencedata = self.enhence_generator.transform(test_mappingdata)\n",
    "        \n",
    "        test_inputdata = np.column_stack((test_mappingdata,test_enhencedata)) \n",
    "        #print('*predictlabel shape:',self.decode(test_inputdata.dot(self.W)).shape)\n",
    "        #print('*predictlabel:', self.decode(test_inputdata.dot(self.W)))\n",
    "        #print('*accuracy:',show_accuracy(self.decode(test_inputdata.dot(self.W)),testlabel))\n",
    "        return self.decode(test_inputdata.dot(self.W))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 21) 2\n",
      "(373, 21) (373,) (125, 21) (125,)\n",
      "maptiems:15\tenhancetimes:15\tk_average_acc:0.83122\tk_average_f1:0.82843\tk_average_auc:0.58381\tk_average_recall:0.83122\tk_average_mcc:0.15188\tk_average_Gm:0.48591\t\n",
      "maptiems:15\tenhancetimes:20\tk_average_acc:0.81778\tk_average_f1:0.82208\tk_average_auc:0.57509\tk_average_recall:0.81778\tk_average_mcc:0.12629\tk_average_Gm:0.48187\t\n",
      "maptiems:15\tenhancetimes:25\tk_average_acc:0.82034\tk_average_f1:0.82141\tk_average_auc:0.58694\tk_average_recall:0.82034\tk_average_mcc:0.1353\tk_average_Gm:0.49222\t\n",
      "maptiems:15\tenhancetimes:30\tk_average_acc:0.82055\tk_average_f1:0.81979\tk_average_auc:0.56485\tk_average_recall:0.82055\tk_average_mcc:0.10868\tk_average_Gm:0.46465\t\n",
      "maptiems:15\tenhancetimes:35\tk_average_acc:0.83371\tk_average_f1:0.83636\tk_average_auc:0.55373\tk_average_recall:0.83371\tk_average_mcc:0.1353\tk_average_Gm:0.4364\t\n",
      "maptiems:20\tenhancetimes:15\tk_average_acc:0.83649\tk_average_f1:0.83544\tk_average_auc:0.54718\tk_average_recall:0.83649\tk_average_mcc:0.12919\tk_average_Gm:0.42641\t\n",
      "maptiems:20\tenhancetimes:20\tk_average_acc:0.84737\tk_average_f1:0.84513\tk_average_auc:0.62543\tk_average_recall:0.84737\tk_average_mcc:0.2281\tk_average_Gm:0.5588\t\n",
      "maptiems:20\tenhancetimes:25\tk_average_acc:0.85526\tk_average_f1:0.85267\tk_average_auc:0.63673\tk_average_recall:0.85526\tk_average_mcc:0.24701\tk_average_Gm:0.57495\t\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(\"../../dataset/NASA/CM1.csv\")  \n",
    "\n",
    "    # Encode categorical data\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for item in data.columns:\n",
    "        data[item] = le.fit_transform(data[item])\n",
    "\n",
    "    # Separate labels from features\n",
    "    label = data['defects'].values\n",
    "    data = data.drop('defects', axis=1).values\n",
    "\n",
    "    print(data.shape, max(label) + 1)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    traindata, testdata, trainlabel, testlabel = train_test_split(data, label, test_size=0.25, random_state=0)\n",
    "    print(traindata.shape, trainlabel.shape, testdata.shape, testlabel.shape)\n",
    "\n",
    "    # Initialize lists to store evaluation metrics\n",
    "    k_acc_list, k_f1_list, k_auc_list, k_recall_list, k_mcc_list, k_Gm_list = [], [], [], [], [], []\n",
    "\n",
    "    # Set up KFold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Iterate over different map and enhance times\n",
    "    for map_times in np.arange(15, 36, 5):\n",
    "        for enhance_times in np.arange(15, 36, 5):\n",
    "            acc_list_tmp, f1_list_tmp, auc_list_tmp, recall_list_tmp, mcc_list_tmp, Gm_list_tmp = [], [], [], [], [], []\n",
    "            \n",
    "            for k, (train, test) in enumerate(kf.split(traindata, trainlabel)):\n",
    "                k_train_data, k_train_label = traindata[train], trainlabel[train]\n",
    "                k_test_data, k_test_label = traindata[test], trainlabel[test]\n",
    "\n",
    "                # Initialize the broadnet model\n",
    "                bls = broadnet(\n",
    "                    maptimes=map_times,\n",
    "                    enhencetimes=enhance_times,\n",
    "                    map_function='relu',\n",
    "                    enhence_function='relu',\n",
    "                    batchsize=100,\n",
    "                    reg=0.001\n",
    "                )\n",
    "\n",
    "                # Train the model\n",
    "                starttime = datetime.datetime.now()\n",
    "                bls.fit(k_train_data, k_train_label)\n",
    "                endtime = datetime.datetime.now()\n",
    "\n",
    "                # Predict labels\n",
    "                k_predict_label = bls.predict(k_test_data)\n",
    "\n",
    "                # Calculate evaluation metrics\n",
    "                acc = accuracy_score(k_test_label, k_predict_label, normalize=True)\n",
    "                fmeasure = f1_score(k_test_label, k_predict_label, average='weighted', labels=np.unique(k_test_label))\n",
    "                try:\n",
    "                    auc = roc_auc_score(k_test_label, k_predict_label, average='weighted')\n",
    "                except ValueError:\n",
    "                    auc = np.nan  # Handle cases where AUC can't be computed\n",
    "\n",
    "                recall = recall_score(k_test_label, k_predict_label, average='weighted')\n",
    "                MCC = matthews_corrcoef(k_test_label, k_predict_label)\n",
    "                Gmeasure = geometric_mean_score(k_test_label, k_predict_label, average='weighted')\n",
    "\n",
    "                # Append metrics to temporary lists\n",
    "                acc_list_tmp.append(acc)\n",
    "                f1_list_tmp.append(fmeasure)\n",
    "                auc_list_tmp.append(auc)\n",
    "                recall_list_tmp.append(recall)\n",
    "                mcc_list_tmp.append(MCC)\n",
    "                Gm_list_tmp.append(Gmeasure)\n",
    "                \n",
    "            # Calculate and store mean values for each metric\n",
    "            k_acc_list.append(round(np.nanmean(acc_list_tmp), 5))\n",
    "            k_f1_list.append(round(np.nanmean(f1_list_tmp), 5))\n",
    "            k_auc_list.append(round(np.nanmean(auc_list_tmp), 5))\n",
    "            k_recall_list.append(round(np.nanmean(recall_list_tmp), 5))\n",
    "            k_mcc_list.append(round(np.nanmean(mcc_list_tmp), 5))\n",
    "            k_Gm_list.append(round(np.nanmean(Gm_list_tmp), 5))\n",
    "\n",
    "            # Print the results\n",
    "            print(f'maptiems:{map_times}\\tenhancetimes:{enhance_times}\\tk_average_acc:{k_acc_list[-1]}\\tk_average_f1:{k_f1_list[-1]}\\tk_average_auc:{k_auc_list[-1]}\\tk_average_recall:{k_recall_list[-1]}\\tk_average_mcc:{k_mcc_list[-1]}\\tk_average_Gm:{k_Gm_list[-1]}\\t')\n",
    "\n",
    "    # Reshape lists to arrays\n",
    "    reshape_size = len(np.arange(15, 36, 5))\n",
    "    k_acc_array = np.array(k_acc_list).reshape(reshape_size, reshape_size)\n",
    "    k_f1_array = np.array(k_f1_list).reshape(reshape_size, reshape_size)\n",
    "    k_auc_array = np.array(k_auc_list).reshape(reshape_size, reshape_size)\n",
    "    k_recall_array = np.array(k_recall_list).reshape(reshape_size, reshape_size)\n",
    "    k_mcc_array = np.array(k_mcc_list).reshape(reshape_size, reshape_size)\n",
    "    k_Gm_array = np.array(k_Gm_list).reshape(reshape_size, reshape_size)\n",
    "\n",
    "    # Save results to .mat file\n",
    "    sio.savemat('./data_remember/0.75BLS_CM1_MEbest.mat', {\n",
    "        'acc': k_acc_array,\n",
    "        'f1': k_f1_array,\n",
    "        'auc': k_auc_array,\n",
    "        'recall': k_recall_array,\n",
    "        'mcc': k_mcc_array,\n",
    "        'Gm': k_Gm_array\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc=accuracy_score(predictlabel, testlabel, normalize=True)\n",
    "# precision=precision_score(predictlabel, testlabel, average='weighted',zero_division=1)\n",
    "# recall=recall_score(predictlabel, testlabel, average='weighted')\n",
    "# fmeasure=f1_score(predictlabel, testlabel, average='weighted', labels=np.unique(testlabel))\n",
    "# auc=roc_auc_score(predictlabel, testlabel, average='weighted', sample_weight=None)\n",
    "# MCC=matthews_corrcoef(predictlabel, testlabel)\n",
    "# Gmeasure=geometric_mean_score(predictlabel, testlabel, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('acc：%f,precision：%f,recall：%f,fmeasure：%f,auc：%f,MCC：%f,Gmeasure：%f'%(acc,precision,recall,fmeasure,auc,MCC,Gmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
